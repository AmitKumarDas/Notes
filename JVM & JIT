  JIT Compilation
--------------------
  > transforms code
  > code actually executed can be different than the code you write

  Compiler Tricks - Reads can be cached
----------------------------------------
  int distanceRatio(Object a) {
    int distanceTo = a.getX() - start;
    int distanceAfter = end - a.getX();
    return distanceTo/distanceAfter;
}

  Is same as 

  int distanceRatio(Object a) {
    int x = a.getX(); 
    int distanceTo = x - start; 
    int distanceAfter = end - x;
    return distanceTo/distanceAfter;
}

   - Compiler Tricks - Reads can be cached
----------------------------------------
  void loopUntilFlagSet(Object a) {
    while (!a.ﬂagIsSet()) {
    loopcount++;
  }
}

  Is same as:

  void loopUntilFlagSet(Object a) {
    boolean ﬂagIsSet = a.ﬂagIsSet();
    while (!ﬂagIsSet) {
      loopcount++;
  }
}


  Compiler Tricks 
-----------------------------------------------
  - Writes can be eliminated
  - Inlining

  public class Thing {
    private int x;
    public ﬁnal int getX() { return x };
  }
  ...
  myX = thing.getX();

  Is same as 

  Class Thing {
    int x;
  }
  ...
  myX = thing.x;


  JIT can do CHA
------------------
  > Class Hierarchy Analysis
  
  Adapting to Adaptive JIT
------------------------------
  > Trading system wants to have the first trade be fast
  > so run 20,000 fake messages thru the system to warm up
  > just make it real enough with proper source & sink

  
- JVM/JIT & Memory Fence Instructions
----------------------------------------
 - If the compiler generated bytecode that obeys the Java memory model, 
 - then the JVM/JIT will issue memory fence instructions when necessary - 
 - when reading/writing volatile variables, acquiring or releasing the monitors on objects etc.

  
 Memory Barriers/Fencing
---------------------------
 - set of processor instructions
 - to apply ordering limitations on memory ops
 
 Memory Barriers & Determinism of multi-threaded programs
------------------------------------------------------------
 - A main memory operation costs hundreds of clock cycles on commodity hardware. 
 - Processors use caching to decrease the costs of memory latency by orders of magnitude. 
 - These caches re-order pending memory operations for the sake of performance. 
 - In other words, the reads and writes of a program are not necessarily performed in the order 
 - - in which they are given to the processor. 
 - When data is immutable and/or confined to the scope of one thread these optimizations are harmless.

 - Memory Barriers & JVM
--------------------------
 - MBs r not exposed by JVM
 - 




References
---------------
1/ http://www.infoq.com/presentations/JVM-Mechanics


